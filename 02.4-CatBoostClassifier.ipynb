{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATBOOST CLASSIFIER ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Train, Valid, Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from my_libs import lib_tools as pt\n",
    "\n",
    "# run_type = 'dev'\n",
    "run_type = 'prd'\n",
    "gen_sample = False\n",
    "find_best_params = False\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = pt.get_train_valid_test_data(run_type)\n",
    "print(\"Train, Valid and Test data loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample data with SMOTEN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gen_sample:\n",
    "    X_train_rs, y_train_rs = pt.get_data_resampled(X=X_train, y=y_train, verbose=1)\n",
    "    # Save data generated\n",
    "    X_train_rs.to_pickle(f'./pickles/X_train_smote_{run_type}_{X_train.shape[0]}.pkl')\n",
    "    y_train_rs.to_pickle(f'./pickles/y_train_smote_{run_type}_{X_train.shape[0]}.pkl')\n",
    "else:\n",
    "    # Load data previously generated\n",
    "    X_train_rs = pd.read_pickle(f'./pickles/X_train_smote_{run_type}_{X_train.shape[0]}.pkl')\n",
    "    y_train_rs = pd.read_pickle(f'./pickles/y_train_smote_{run_type}_{X_train.shape[0]}.pkl')\n",
    "\n",
    "pt.plot_data_augmentation(y_train, y_train_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best hyperparameters for model with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "if find_best_params:\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        dt_iterations    = trial.suggest_int('iterations', 50, 300, log=True)\n",
    "        dt_learning_rate = trial.suggest_float('learning_rate', 0.001, 0.1, log=True)\n",
    "\n",
    "        classifier_obj = CatBoostClassifier(iterations=dt_iterations, learning_rate=dt_learning_rate, cat_features=list(X_train.columns), verbose=0)\n",
    "        score = cross_val_score(classifier_obj, X_train_rs, y_train_rs, cv=5, scoring=\"f1\", verbose=1)\n",
    "        accuracy = score.mean()\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "\n",
    "    print(\"--- CatBoost Classifier - Optimization with Optuna performed in %s seconds ---\" % (time.time() - start_time))\n",
    "    print(f\"Best params : {study.best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if find_best_params:\n",
    "    from optuna.visualization import plot_optimization_history\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_libs.model_evaluator import ModelEvaluator\n",
    "\n",
    "if find_best_params:\n",
    "\n",
    "    params = study.best_params\n",
    "    params['cat_features'] = list(X_train.columns)\n",
    "\n",
    "    evaluator = ModelEvaluator(model_type='CatBoostClassifier', params=params, X_train=X_train_rs, y_train=y_train_rs, X_test=X_test, y_test=y_test)\n",
    "    model = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit best model - Plot Train and Test learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Train the best model\n",
    "params = study.best_params\n",
    "params['cat_features'] = list(X_train.columns)\n",
    "model = CatBoostClassifier(**params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute the learning curve\n",
    "train_sizes, train_scores, valid_scores = learning_curve(model, X_train_rs, y_train_rs, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# Calculate the mean and standard deviation of the training and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std  = np.std(train_scores, axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "valid_scores_std  = np.std(valid_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Catboost Classifier Learning Curve')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color='r')\n",
    "plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color='g')\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training Score')\n",
    "plt.plot(train_sizes, valid_scores_mean, 'o-', color='g', label='Validation Score')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ROC curves (from estimator & from predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "RocCurveDisplay.from_estimator(model, X_test, y_test)\n",
    "plt.plot(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01))\n",
    "plt.title('Catboost - ROC Curve from estimator')\n",
    "\n",
    "RocCurveDisplay.from_predictions(y_test, model.predict(X_test))\n",
    "plt.plot(np.arange(0, 1, 0.01), np.arange(0, 1, 0.01))\n",
    "plt.title('Catboost - ROC Curve from predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change proba threshold to improve f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, plot_roc_curve, f1_score, precision_score, recall_score\n",
    "# plot_roc_curve(model, X_train, y_train)\n",
    "\n",
    "thresholds = np.arange(0.20, 0.80, 0.01)\n",
    "scores_f1 = []\n",
    "scores_prec = []\n",
    "scores_recall = []\n",
    "for k in thresholds:\n",
    "    y_pred = (model.predict_proba(X_test)[:,1] >= k).astype(bool)\n",
    "    scores_f1.append(f1_score(y_test, y_pred))\n",
    "    scores_prec.append(precision_score(y_test, y_pred))\n",
    "    scores_recall.append(recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(2,1, figsize=(7,10))\n",
    "plt.grid()\n",
    "axs[0].plot(thresholds, scores_f1, label='f1')\n",
    "axs[0].set_title(\"Valeur du f1-score en fonction du seuil de probabilité d'attribution des classes\")\n",
    "axs[0].grid()\n",
    "axs[1].plot(thresholds, scores_f1, label='f1')\n",
    "axs[1].plot(thresholds, scores_recall, label='recall')\n",
    "axs[1].plot(thresholds, scores_prec, label='precision')\n",
    "axs[1].set_title(\"Valeur des métriques en fonction du seuil de probabilité d'attribution des classes\")\n",
    "axs[1].grid()\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred = (model.predict_proba(X_test)[:,1] >= 0.42).astype(bool)\n",
    "\n",
    "display(pd.crosstab(y_test, y_pred, rownames=['Classe réelle'], colnames=['Classe prédite']))\n",
    "\n",
    "print(\"\\nClassification report :\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapash import SmartExplainer\n",
    "from my_libs import ref_labels\n",
    "import pickle\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "xpl = SmartExplainer(\n",
    "    model=model,\n",
    "    label_dict={0: 'Indemne - Blessé léger', 1: 'Tué - Blessé hospitalisé'},\n",
    "    preprocessing=ref_labels.dic_preproc,\n",
    "    features_dict=ref_labels.dic_features,  # Optional parameter\n",
    "    # preprocessing=encoder, # Optional: compile step can use inverse_transform method\n",
    "    # postprocessing=postprocess # Optional: see tutorial postprocessing\n",
    ")\n",
    "\n",
    "y_test.index = X_test.index\n",
    "\n",
    "xpl.compile(\n",
    "    x=X_test,\n",
    "    # y_pred=y_pred, # Optional: for your own prediction (by default: model.predict)\n",
    "    y_target=y_test, # Optional: allows to display True Values vs Predicted Values\n",
    ")\n",
    "\n",
    "app = xpl.run_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.kill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model to h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "# Save the model to an h5 file using joblib\n",
    "dump(model, f'h5_models/model_cb_{run_type}_{X_train.shape[0]}.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
