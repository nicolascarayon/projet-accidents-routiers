{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509685c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import lib_data_load as ld\n",
    "import lib_data_ref as dr\n",
    "from datetime import date\n",
    "import seaborn as sns\n",
    "\n",
    "start_year = 2005\n",
    "end_year = 2021\n",
    "\n",
    "# load data into dictionnaries\n",
    "dic_usagers = ld.load_usagers(start_year, end_year)\n",
    "dic_caract = ld.load_caract(start_year, end_year)\n",
    "dic_vehic = ld.load_vehicules(start_year=start_year, end_year=end_year)\n",
    "dic_lieux = ld.load_lieux(start_year=start_year, end_year=end_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a09bb9",
   "metadata": {},
   "source": [
    "### Prepare df_usagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d20097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all df usagers\n",
    "# from 2005 to 2018 -> ['secu'] is splitted into ['secu1', 'secu2', 'secu3']\n",
    "for year in range(start_year, end_year+1):\n",
    "    df = dic_usagers[year]    \n",
    "    if 2005 <= year <= 2018:\n",
    "        # create columns ['secu1', 'secu2', 'secu3'] and drop old 'secu'\n",
    "        df['secu'] = df['secu'].replace(to_replace=np.nan, value=-1)\n",
    "        df['secu1'] = df['secu'] // 10\n",
    "        df['secu2'] = df['secu'] % 10\n",
    "        df['secu3'] = np.ones(len(df['secu']))*(-1)\n",
    "\n",
    "        df = df.drop(columns=['secu'])\n",
    "        df['secu1'] = df['secu1'].astype('int')\n",
    "        df['secu2'] = df['secu2'].astype('int')\n",
    "        df['secu3'] = df['secu3'].astype('int')\n",
    "    \n",
    "    if year == start_year:\n",
    "        df_usagers = df\n",
    "    else:\n",
    "        df_usagers = pd.concat([df_usagers, df], axis=0)\n",
    "\n",
    "# check number of lines\n",
    "nb_lines = 0\n",
    "for year in range(start_year, end_year+1):\n",
    "    nb_lines += dic_usagers[year].shape[0]\n",
    "    \n",
    "print('somme des lignes :', nb_lines)\n",
    "print('nb de lignes de df_usagers', df_usagers.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c4f489",
   "metadata": {},
   "source": [
    "### Prepare df_caract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year+1):\n",
    "    df = dic_caract[year]\n",
    "    if 'gps' in df.columns:\n",
    "        df = df.drop(columns=['gps'], axis=1)\n",
    "    if year == start_year:\n",
    "        df_caract = df\n",
    "    else:\n",
    "        df_caract = pd.concat([df_caract, df], ignore_index=True, axis=0)\n",
    "\n",
    "df_caract['an'] = df_caract['an'].replace(to_replace=[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], \n",
    "                                          value=[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, \n",
    "                                                 2015, 2016, 2017, 2018])\n",
    "        \n",
    "        \n",
    "# check number of lines\n",
    "nb_lines = 0\n",
    "for year in range(start_year, end_year+1):\n",
    "    nb_lines += dic_caract[year].shape[0]\n",
    "\n",
    "print('somme des lignes :', nb_lines)\n",
    "print('nb de lignes de df_caract', df_caract.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8288e54",
   "metadata": {},
   "source": [
    "### Prepare df_vehic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehic = pd.DataFrame(columns=['Num_Acc', 'id_vehicule', 'num_veh', 'senc', 'catv', 'obs', 'obsm', \n",
    "                                 'choc', 'manv', 'motor', 'occutc'])\n",
    "\n",
    "for year in range(start_year, end_year+1):\n",
    "    df = dic_vehic[year]\n",
    "    if year == start_year:\n",
    "        df_vehic = df\n",
    "    else:\n",
    "        df_vehic = pd.concat([df_vehic, df], ignore_index=True, axis=0)\n",
    "        \n",
    "# check number of lines\n",
    "nb_lines = 0\n",
    "for year in range(start_year, end_year+1):\n",
    "    nb_lines += dic_vehic[year].shape[0]\n",
    "\n",
    "print('somme des lignes :', nb_lines)\n",
    "print('nb de lignes de df_caract', df_vehic.shape[0])\n",
    "\n",
    "# for col in df_vehic.columns:\n",
    "#     print(f'{col} : ', df_vehic[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217241c5",
   "metadata": {},
   "source": [
    "### Prepare df_lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for year in range(start_year, end_year+1):\n",
    "#     df = dic_lieux[year]\n",
    "#     print(f'{year : }', df.columns)\n",
    "\n",
    "for year in range(start_year, end_year+1):\n",
    "    df = dic_lieux[year]\n",
    "    if year == start_year:\n",
    "        df_lieux = df\n",
    "    else:\n",
    "        df_lieux = pd.concat([df_lieux, df], ignore_index=True, axis=0)\n",
    "        \n",
    "# check number of lines\n",
    "nb_lines = 0\n",
    "for year in range(start_year, end_year+1):\n",
    "    nb_lines += dic_lieux[year].shape[0]\n",
    "\n",
    "print('somme des lignes :', nb_lines)\n",
    "print('nb de lignes de df_caract', df_lieux.shape[0])\n",
    "\n",
    "# for col in df_lieux.columns:\n",
    "#     print(f'{col} : ', df_lieux[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401834c",
   "metadata": {},
   "source": [
    "###  Check for duplicated and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avant suppression des doublons : \\n\")\n",
    "print(' - usagers en doublons : ', df_usagers.duplicated().sum())\n",
    "print(' - caractéristiques en doublons :', df_caract.duplicated().sum())\n",
    "print(' - véhicules en doublons :', df_vehic.duplicated().sum())\n",
    "print(' - lieux en doublons : ', df_lieux.duplicated().sum())\n",
    "\n",
    "df_usagers.drop_duplicates(inplace=True)\n",
    "df_caract.drop_duplicates(inplace=True)\n",
    "df_vehic.drop_duplicates(inplace=True)\n",
    "df_lieux.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456345c",
   "metadata": {},
   "source": [
    "### Merge data into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_usagers.merge(on=['Num_Acc'], right=df_caract, how='left')\n",
    "df = df.merge(on=['Num_Acc', 'id_vehicule', 'num_veh'], right=df_vehic, how='left')\n",
    "df = df.merge(on='Num_Acc', right=df_lieux, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1039024",
   "metadata": {},
   "source": [
    "### Check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4853b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_caract.sample(10000)\n",
    "df_sample = df_sample.sort_values(by='Num_Acc', ascending=True)\n",
    "sns.heatmap(df_sample.isna());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484e6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(10000)\n",
    "df_sample = df_sample.sort_values(by='Num_Acc', ascending=True)\n",
    "ax = sns.heatmap(df_sample.isna())\n",
    "ax.axes.yaxis.set_ticklabels('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b004498",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_usagers.sample(10000)\n",
    "df_sample = df_sample.sort_values(by='Num_Acc', ascending=True)\n",
    "sns.heatmap(df_sample.isna());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_lieux.sample(10000)\n",
    "df_sample = df_sample.sort_values(by='Num_Acc', ascending=True)\n",
    "sns.heatmap(df_sample.isna());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4be08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_vehic.sample(10000)\n",
    "df_sample = df_sample.sort_values(by='Num_Acc', ascending=True)\n",
    "sns.heatmap(df_sample.isna());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efaed0",
   "metadata": {},
   "source": [
    "### Volume des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb de lignes et colonnes de chaque DataFrame\n",
    "\n",
    "dic = {'caract' : dic_caract, 'lieux' : dic_lieux, 'usagers': dic_usagers, 'vehic' : dic_vehic}\n",
    "\n",
    "for key in dic.keys() : \n",
    "    print(f\"\\n{key} : \\n\")\n",
    "    nb_lin = []\n",
    "    nb_col = []\n",
    "   \n",
    "    for year in range(start_year, end_year+1):\n",
    "        dic_data = dic[key]\n",
    "        df = dic_data[year]\n",
    "        nb_lin.append(df.shape[0])\n",
    "        nb_col.append(df.shape[1])\n",
    "        print(f'{key} {year} : {df.shape[1]} colonnes x {df.shape[0]} lignes')\n",
    "\n",
    "    print(f\"\\nnombre de lignes min : {min(nb_lin)}\")    \n",
    "    print(f\"\\nnombre de lignes max : {max(nb_lin)}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c337b",
   "metadata": {},
   "source": [
    "### Analyse intercorrelations between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafc60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['grav']\n",
    "data = df.drop(columns=['grav'], axis=1)\n",
    "# set 'grav' as last column\n",
    "data['grav'] = target\n",
    "\n",
    "data = data.fillna(data.median())\n",
    "# data_clean = data.dropna(how='any')\n",
    "\n",
    "data_sample = data.sample(10000)\n",
    "corr = data_sample.corr()\n",
    "\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm');\n",
    "sns.clustermap(corr, annot=False, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519b6ba",
   "metadata": {},
   "source": [
    "### Existence de Null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1770611",
   "metadata": {},
   "source": [
    "### Evolution de gravité 'Blessé hospitalisé' en 2019?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f824e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2018 = df_usagers[df_usagers['Num_Acc'].astype('str').str[:4] == '2018']\n",
    "df_usagers_2019 = df_usagers[df_usagers['Num_Acc'].astype('str').str[:4] == '2019']\n",
    "\n",
    "# sns.histplot(df_usagers_2018.grav)\n",
    "# ax = plt.hist([df_usagers_2018.grav, df_usagers_2019.grav], color=['r', 'b'], alpha=0.5)\n",
    "\n",
    "\n",
    "print(\"\\nDistribution de la variable gravité en 2018 : \\n\")\n",
    "print(df_usagers_2018.grav.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDistribution de la variable gravité en 2019 : \\n\")\n",
    "print(df_usagers_2019.grav.value_counts(normalize=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
