{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "quick = True\n",
    "\n",
    "if quick:\n",
    "    start_year, end_year, chk, sampled, filename = [2005, 2021, False, True, 'df-light.pkl']\n",
    "else:\n",
    "    start_year, end_year, chk, sampled, filename = [2005, 2021, True, False, 'df-full.pkl']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_pickle(f'./{filename}')\n",
    "data = df.iloc[:, 1:]\n",
    "target = df['grav']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=222)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "12 fits failed out of a total of 36.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "12 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 1085, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 901, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 819, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\_parallel_backends.py\", line 597, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Nicolas\\AppData\\Roaming\\Python\\Python39\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.91834421 0.92123381 0.92228089 0.92098219 0.9175244  0.92073868\n",
      " 0.92091724 0.92116888        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score  :  0.9222808863774564\n",
      "Best params :  {'criterion': 'gini', 'n_estimators': 50}\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.62      0.91      0.44      0.74      0.63      0.42     15490\n",
      "          1       0.83      0.44      0.91      0.58      0.63      0.38     15490\n",
      "\n",
      "avg / total       0.73      0.68      0.68      0.66      0.63      0.40     30980\n",
      "\n",
      "--- performed in 169.10041737556458 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from category_encoders import TargetEncoder, OneHotEncoder\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cols_target_encoded = ['dep', 'age']\n",
    "cols_onehot_encoded = X_train.columns.drop(cols_target_encoded)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "encoder_target = TargetEncoder(cols=cols_target_encoded)\n",
    "encoder_onehot = OneHotEncoder(cols=cols_onehot_encoded)\n",
    "# sampler        = SMOTE(random_state=42)\n",
    "# sampler        = RandomUnderSampler()\n",
    "sampler        = RandomOverSampler()\n",
    "model          = RandomForestClassifier()\n",
    "\n",
    "\n",
    "X_train_te = encoder_target.fit_transform(X_train, y_train)\n",
    "X_test_te  = encoder_target.transform(X_test)\n",
    "\n",
    "X_train_oh = encoder_onehot.fit_transform(X_train_te, y_train)\n",
    "X_test_oh  = encoder_onehot.transform(X_test_te)\n",
    "\n",
    "X_train_sc = scaler.fit_transform(X_train_oh)\n",
    "X_test_sc  = scaler.transform(X_test_oh)\n",
    "\n",
    "X_train_rs, y_train_rs = sampler.fit_resample(X_train_sc, y_train)\n",
    "X_test_rs, y_test_rs   = sampler.fit_resample(X_test_sc, y_test)\n",
    "\n",
    "params = {\n",
    "    'n_estimators' : [10, 20, 50, 70],\n",
    "    'criterion' : ('gini', 'entropy', 'log_loss'),\n",
    "    # 'max_depth' : (None, 10, 50),\n",
    "    # 'min_samples_split' : (2,4,6)\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, cv = 3, n_jobs=-1)\n",
    "grid.fit(X_train_rs, y_train_rs)\n",
    "\n",
    "print('Best score  : ', grid.best_score_)\n",
    "print('Best params : ', grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_test_rs)\n",
    "print(classification_report_imbalanced(y_test_rs, y_pred))\n",
    "\n",
    "print(\"--- performed in %s seconds ---\" % (time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
